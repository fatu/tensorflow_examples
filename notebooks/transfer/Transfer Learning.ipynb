{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/drivers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Transfer learning has benefitted many real-world applications where labeled data are abundant in source domains but scarce on the target domain. \n",
    "2. Speed up training process. Target/Domain e.g. Dogs vs. Cats https://www.kaggle.com/c/dogs-vs-cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labeled source data and labeled target data: **Fine tuning**, **Multitask Learning**  \n",
    "Labeled source data and Un-labeled target data: **Domain-adversarial training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "1. We use ImageNet pre-trained model. inception-resnet-v2.\n",
    "2. We fine-tune that inception-resnet-v2 to transfer generic learned features to do flowers classification.\n",
    "3. Lower layers of the inception-resnet-v2 contains generic features that can be used for the flowers classification.\n",
    "https://github.com/kwotsin/transfer_learning_tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Conservative training\n",
    "Problem: Strong DL model can be overfiting for some small targets. Too many weights.   \n",
    "Solution: Regularization  \n",
    "The weights are close:  \n",
    "1. Learning rate. \n",
    "2. Loss Function ||w1-w1'||\n",
    "\n",
    "### 1.2 Layer Transfer\n",
    "1. Freeze top layers: image, speech(phoneme).\n",
    "2. Data enough, fine-tuning whole model.\n",
    "![title](img/elephent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multi-task learning (MTL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A subfield of machine learning which multiple learning tasks are solved at the same time, while exploiting commonalities and differences across tasks. \n",
    "![title](img/multi_task.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Domain-adversarial training\n",
    "Neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are :  \n",
    "(i) discriminative for the main learning task on the source domain   \n",
    "(ii) indiscriminate with respect to the shift between the domains. \n",
    "\n",
    "![title](img/mnist-m.png)\n",
    "![title](img/domain_adversarial_training.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
